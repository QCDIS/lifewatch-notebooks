{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "854c645f-a14b-49ca-a9f5-cb49de30006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "import laspy\n",
    "\n",
    "\n",
    "import time\n",
    "import requests\n",
    "                    \n",
    "from dask.distributed import LocalCluster, SSHCluster \n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32a2c7-6bc3-4a23-a7f3-be6163358654",
   "metadata": {},
   "source": [
    "## Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efe9bea7-2750-4882-9dc5-5e4b4adbf273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations spiros\n",
    "\n",
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "                    \n",
    "from dask.distributed import LocalCluster, SSHCluster \n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote\n",
    "\n",
    "conf_remote_path_root = pathlib.Path('/webdav')\n",
    "conf_remote_path_ahn = pathlib.Path('/webdav/ahn')\n",
    "conf_remote_path_split = pathlib.Path('/webdav/split')\n",
    "conf_remote_path_retiled = pathlib.Path('/webdav/retiled/')\n",
    "conf_remote_path_norm = pathlib.Path('/webdav/norm/')\n",
    "conf_remote_path_targets = pathlib.Path('/webdav/targets')\n",
    "conf_local_tmp = pathlib.Path('/tmp')\n",
    "\n",
    "\n",
    "param_hostname = ''\n",
    "param_login = ''\n",
    "param_password = ''\n",
    "\n",
    "param_feature_name = 'perc_95_normalized_height'\n",
    "param_validate_precision = '0.00001'\n",
    "param_tile_mesh_size = '10.'\n",
    "param_filter_type= 'select_equal'\n",
    "param_attribute = 'raw_classification'\n",
    "param_min_x = '-113107.81'\n",
    "param_max_x = '398892.19'\n",
    "param_min_y = '214783.87'\n",
    "param_max_y = '726783.87'\n",
    "param_n_tiles_side = '512'\n",
    "param_apply_filter_value = '1'\n",
    "param_laz_compression_factor = '7'\n",
    "param_max_filesize = '262144000'  # desired max file size (in bytes)\n",
    "\n",
    "conf_wd_opts = { 'webdav_hostname': param_hostname, 'webdav_login': param_login, 'webdav_password': param_password}\n",
    "\n",
    "\n",
    "param_grafana_base_url = ''\n",
    "param_grafana_token = ''\n",
    "\n",
    "conf_notebook_name = ''\n",
    "conf_grafana_verify_ssl = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70913967-81e0-4b22-8b4d-aa1b3f5f2707",
   "metadata": {},
   "source": [
    "## Fetching Laz Files from remote WebDAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30c8f77b-e28a-4b3b-aa29-5b18619251a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"id\":3,\"message\":\"Annotation added\"}'\n"
     ]
    }
   ],
   "source": [
    "# Fetch Laz Files 01-06-22\n",
    "\n",
    "def send_annotation(start=None,end=None,message=None,tags=None):\n",
    "    if not tags:\n",
    "        tags = []\n",
    "    \n",
    "    tags.append(conf_notebook_name)\n",
    "    \n",
    "    headers = {\n",
    "        'Accept':'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': 'Bearer '+param_grafana_token\n",
    "    }\n",
    "    \n",
    "    data ={\n",
    "      \"time\":start,\n",
    "      \"timeEnd\":end,\n",
    "      \"created\": end,\n",
    "      \"tags\":tags,\n",
    "      \"text\": message\n",
    "    }\n",
    "    resp = requests.post(param_grafana_base_url+'/api/annotations',verify=conf_grafana_verify_ssl,headers=headers,json=data)\n",
    "\n",
    "\n",
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "laz_files = [f for f in list_remote(get_wdclient(conf_wd_opts), conf_remote_path_ahn.as_posix())\n",
    "             if f.lower().endswith('.laz')]\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message='Fetch Laz Files 01-06-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0973e-42e3-47c9-9382-8e30f2086771",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Splitting big files into smaller files before retiling\n",
    "This step can be added if the original files are too large for normal VMs to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c935cbc-02b1-4f0a-a616-c2258a19931d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str, bytes, or os.PathLike object, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m file \u001b[38;5;241m=\u001b[39m laz_files\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# for file in laz_files:\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m client\u001b[38;5;241m.\u001b[39mdownload_sync(remote_path\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf_remote_path_ahn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m, local_path\u001b[38;5;241m=\u001b[39mfile)\n\u001b[1;32m     76\u001b[0m inps \u001b[38;5;241m=\u001b[39m split_strategy(file, \u001b[38;5;28mint\u001b[39m(param_max_filesize))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m inps:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/posixpath.py:90\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     88\u001b[0m             path \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sep \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mBytesWarning\u001b[39;00m):\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mgenericpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_arg_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjoin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/genericpath.py:152\u001b[0m, in \u001b[0;36m_check_arg_types\u001b[0;34m(funcname, *args)\u001b[0m\n\u001b[1;32m    150\u001b[0m         hasbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() argument must be str, bytes, or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    153\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike object, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hasstr \u001b[38;5;129;01mand\u001b[39;00m hasbytes:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt mix strings and bytes in path components\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: join() argument must be str, bytes, or os.PathLike object, not 'list'"
     ]
    }
   ],
   "source": [
    "# split big files 01-60-22\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def send_annotation(start=None,end=None,message=None,tags=None):\n",
    "    if not tags:\n",
    "        tags = []\n",
    "    \n",
    "    tags.append(conf_notebook_name)\n",
    "    \n",
    "    headers = {\n",
    "        'Accept':'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': 'Bearer '+param_grafana_token\n",
    "    }\n",
    "    \n",
    "    data ={\n",
    "      \"time\":start,\n",
    "      \"timeEnd\":end,\n",
    "      \"created\": end,\n",
    "      \"tags\":tags,\n",
    "      \"text\": message\n",
    "    }\n",
    "    resp = requests.post(param_grafana_base_url+'/api/annotations',verify=conf_grafana_verify_ssl,headers=headers,json=data)\n",
    "    \n",
    "\n",
    "def save_chunk_to_laz_file(in_filename, \n",
    "                           out_filename, \n",
    "                           offset, \n",
    "                           n_points):\n",
    "    \"\"\"Read points from a LAS/LAZ file and write them to a new file.\"\"\"\n",
    "    \n",
    "    points = np.array([])\n",
    "    \n",
    "    with laspy.open(in_filename) as in_file:\n",
    "        with laspy.open(out_filename, \n",
    "                        mode=\"w\", \n",
    "                        header=in_file.header) as out_file:\n",
    "            in_file.seek(offset)\n",
    "            points = in_file.read_points(n_points)\n",
    "            out_file.write_points(points)\n",
    "    return len(points)\n",
    "\n",
    "def split_strategy(filename, max_filesize):\n",
    "    \"\"\"Set up splitting strategy for a LAS/LAZ file.\"\"\"\n",
    "    with laspy.open(filename) as f:\n",
    "        bytes_per_point = (\n",
    "            f.header.point_format.num_standard_bytes +\n",
    "            f.header.point_format.num_extra_bytes\n",
    "        )\n",
    "        n_points = f.header.point_count\n",
    "    n_points_target = int(\n",
    "        max_filesize * int(param_laz_compression_factor) / bytes_per_point\n",
    "    )\n",
    "    stem, ext = os.path.splitext(filename)\n",
    "    return [\n",
    "        (filename, f\"{stem}-{n}{ext}\", offset, n_points_target)\n",
    "        for n, offset in enumerate(range(0, n_points, n_points_target))\n",
    "    ]\n",
    "\n",
    "##################### Don't know how to run this sequentially ################################\n",
    "from webdav3.client import Client\n",
    "\n",
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "client = Client(conf_wd_opts)\n",
    "client.mkdir(conf_remote_path_split.as_posix())\n",
    "\n",
    "\n",
    "remote_path_split = conf_remote_path_split\n",
    "\n",
    "file = laz_files\n",
    "# for file in laz_files:\n",
    "\n",
    "client.download_sync(remote_path=os.path.join(conf_remote_path_ahn,file), local_path=file)\n",
    "inps = split_strategy(file, int(param_max_filesize))\n",
    "for inp in inps:\n",
    "    save_chunk_to_laz_file(*inp)\n",
    "client.upload_sync(remote_path=os.path.join(conf_remote_path_split,file), local_path=file)\n",
    "\n",
    "for f in os.listdir('.'):\n",
    "    if not f.endswith('.LAZ'):\n",
    "        continue\n",
    "    os.remove(os.path.join('.', f))\n",
    "    \n",
    "split_laz_files = laz_files\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message='split big files 01-60-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad5d0f-2cbb-4889-ae63-3972626db753",
   "metadata": {},
   "source": [
    "## Retiling of big files into smaller tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034685db-0dda-48f9-98f3-fd38ae2525ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 10:35:16,570 -           laserfarm.pipeline_remote_data -       INFO - Input dir set to /tmp/C_19HZ2.LAZ_input\n",
      "2022-06-02 10:35:16,574 -           laserfarm.pipeline_remote_data -       INFO - Output dir set to /tmp/C_19HZ2.LAZ_output\n",
      "2022-06-02 10:35:16,578 -           laserfarm.pipeline_remote_data -       INFO - Pulling from WebDAV /webdav/split/C_19HZ2.LAZ ...\n",
      "2022-06-02 10:35:17,870 -           laserfarm.pipeline_remote_data -       INFO - ... pulling completed.\n",
      "2022-06-02 10:35:17,872 -                        laserfarm.retiler -       INFO - Setting up the target grid\n",
      "2022-06-02 10:35:17,875 -                        laserfarm.retiler -       INFO - Splitting file /tmp/C_19HZ2.LAZ_input/C_19HZ2.LAZ with PDAL ...\n",
      "2022-06-02 10:35:18,029 -                        laserfarm.retiler -       INFO - ... splitting completed.\n",
      "2022-06-02 10:35:18,031 -                        laserfarm.retiler -       INFO - Redistributing files to tiles ...\n",
      "2022-06-02 10:35:18,032 -                        laserfarm.retiler -       INFO - ... file C_19HZ2_1.LAZ to tile_248_285\n",
      "2022-06-02 10:35:18,033 -                        laserfarm.retiler -       INFO - ... redistributing completed.\n",
      "2022-06-02 10:35:18,034 -                        laserfarm.retiler -       INFO - Validating split ...\n",
      "2022-06-02 10:35:18,036 -                        laserfarm.retiler -       INFO - ... 66663 points in parent file\n",
      "2022-06-02 10:35:18,037 -                        laserfarm.retiler -       INFO - ... 66663 points in C_19HZ2_1.LAZ\n",
      "2022-06-02 10:35:18,038 -                        laserfarm.retiler -       INFO - ... split validation completed.\n",
      "2022-06-02 10:35:18,039 -           laserfarm.pipeline_remote_data -       INFO - Pushing to WebDAV /webdav/retiled ...\n",
      "2022-06-02 10:35:21,536 -           laserfarm.pipeline_remote_data -       INFO - ... pushing completed.\n",
      "2022-06-02 10:35:21,537 -           laserfarm.pipeline_remote_data -       INFO - Removing input and output folders\n"
     ]
    }
   ],
   "source": [
    "# Retiling 01-06-22\n",
    "\n",
    "def send_annotation(start=None,end=None,message=None,tags=None):\n",
    "    if not tags:\n",
    "        tags = []\n",
    "    \n",
    "    tags.append(conf_notebook_name)\n",
    "    \n",
    "    headers = {\n",
    "        'Accept':'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': 'Bearer '+param_grafana_token\n",
    "    }\n",
    "    \n",
    "    data ={\n",
    "      \"time\":start,\n",
    "      \"timeEnd\":end,\n",
    "      \"created\": end,\n",
    "      \"tags\":tags,\n",
    "      \"text\": message\n",
    "    }\n",
    "    resp = requests.post(param_grafana_base_url+'/api/annotations',verify=conf_grafana_verify_ssl,headers=headers,json=data)\n",
    "    \n",
    "    \n",
    "start = int(round(time.time() * 1000))\n",
    "remote_path_retiled = str(conf_remote_path_retiled)\n",
    "\n",
    "grid_retile = {\n",
    "    'min_x': float(param_min_x),\n",
    "    'max_x': float(param_max_x),\n",
    "    'min_y': float(param_min_y),\n",
    "    'max_y': float(param_max_y),\n",
    "    'n_tiles_side': int(param_n_tiles_side)\n",
    "}\n",
    "\n",
    "\n",
    "retiling_input = {\n",
    "    'setup_local_fs': {'tmp_folder': conf_local_tmp.as_posix()},\n",
    "    'pullremote': conf_remote_path_split.as_posix(),\n",
    "    'set_grid': grid_retile,\n",
    "    'split_and_redistribute': {},\n",
    "    'validate': {},\n",
    "    'pushremote': conf_remote_path_retiled.as_posix(),\n",
    "    'cleanlocalfs': {}\n",
    "}\n",
    "\n",
    "\n",
    "# try:\n",
    "#     get_ipython\n",
    "#     file = laz_files[0]\n",
    "# except:\n",
    "#     file = laz_files\n",
    "    \n",
    "# for file in laz_files:\n",
    "file = split_laz_files\n",
    "retiler = Retiler(file.replace('\"',''),label=file).config(retiling_input).setup_webdav_client(conf_wd_opts)\n",
    "retiler_output = retiler.run()\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message='Retiling 01-06-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb76ea-4dfe-4fe1-8533-b88df6e19ef0",
   "metadata": {},
   "source": [
    "## Fetching retilied files (tiles) from remote WebDAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9598142-9943-4bc2-aeae-d0661c5342d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Tiles 01-06-22\n",
    "remote_path_retiled\n",
    "tiles = [t.strip('/') for t in list_remote(get_wdclient(conf_wd_opts), conf_remote_path_retiled.as_posix())\n",
    "         if fnmatch.fnmatch(t, 'tile_*_*/')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4fae4-dad1-4a0f-b606-f8ec2563a2f9",
   "metadata": {},
   "source": [
    "## Normalization - normalize all the point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670a06f-1a41-4909-9a0b-e88c8326a17f",
   "metadata": {},
   "source": [
    "This step is added as the previous notebook did not include this step. The two cells below are the original code deployed on SURF using macroPipline function, so it needs to be modified in order to be containerized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32537f33-f563-4b29-aeb4-507e97588952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 12:02:06,726 -           laserfarm.pipeline_remote_data -       INFO - Input dir set to /tmp/tile_278_391_input\n",
      "2022-05-25 12:02:06,727 -           laserfarm.pipeline_remote_data -       INFO - Output dir set to /tmp/tile_278_391_output\n",
      "2022-05-25 12:02:06,730 -           laserfarm.pipeline_remote_data -       INFO - Pulling from WebDAV /webdav/retiled/tile_278_391 ...\n",
      "2022-05-25 12:02:09,637 -           laserfarm.pipeline_remote_data -       INFO - ... pulling completed.\n",
      "2022-05-25 12:02:09,639 -                laserfarm.data_processing -       INFO - Loading point cloud data ...\n",
      "2022-05-25 12:02:09,640 -                laserfarm.data_processing -       INFO - ... loading /tmp/tile_278_391_input/tile_278_391/C_01GN2_1.LAZ\n",
      "2022-05-25 12:02:12,979 -                laserfarm.data_processing -       INFO - ... loading completed.\n",
      "2022-05-25 12:02:12,981 -                laserfarm.data_processing -       INFO - Normalizing point-cloud heights ...\n",
      "2022-05-25 12:02:13,855 -                                     root -       INFO - Cylinder size in Bytes: 3056136558.191318\n",
      "2022-05-25 12:02:13,856 -                                     root -       INFO - Memory size in Bytes: 16819240960\n",
      "2022-05-25 12:02:13,857 -                                     root -       INFO - Start tree creation\n",
      "2022-05-25 12:02:14,318 -                                     root -       INFO - Done with env tree creation\n",
      "2022-05-25 12:02:14,403 -                                     root -       INFO - Done with target tree creation\n",
      "2022-05-25 12:02:33,126 -                laserfarm.data_processing -       INFO - ... normalization completed.\n",
      "2022-05-25 12:02:33,128 -                laserfarm.data_processing -       INFO - Filtering point-cloud data\n",
      "2022-05-25 12:02:33,700 -                laserfarm.data_processing -       INFO - Exporting environment point-cloud ...\n",
      "2022-05-25 12:02:33,702 -                laserfarm.data_processing -       INFO - ... exporting /tmp/tile_278_391_output/tile_278_391.laz\n",
      "2022-05-25 12:02:35,247 -                laserfarm.data_processing -       INFO - ... exporting completed.\n",
      "2022-05-25 12:02:35,249 -                laserfarm.data_processing -       INFO - Clearing cached KDTrees ...\n",
      "2022-05-25 12:02:35,252 -           laserfarm.pipeline_remote_data -       INFO - Pushing to WebDAV /webdav/norm ...\n",
      "2022-05-25 12:02:39,674 -           laserfarm.pipeline_remote_data -       INFO - ... pushing completed.\n"
     ]
    }
   ],
   "source": [
    "# normalization 01-06-22\n",
    "import copy\n",
    "\n",
    "def send_annotation(start=None,end=None,message=None,tags=None):\n",
    "    if not tags:\n",
    "        tags = []\n",
    "    \n",
    "    tags.append(conf_notebook_name)\n",
    "    \n",
    "    headers = {\n",
    "        'Accept':'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': 'Bearer '+param_grafana_token\n",
    "    }\n",
    "    \n",
    "    data ={\n",
    "      \"time\":start,\n",
    "      \"timeEnd\":end,\n",
    "      \"created\": end,\n",
    "      \"tags\":tags,\n",
    "      \"text\": message\n",
    "    }\n",
    "    resp = requests.post(param_grafana_base_url+'/api/annotations',verify=conf_grafana_verify_ssl,headers=headers,json=data)\n",
    "\n",
    "\n",
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "\n",
    "tiles\n",
    "\n",
    "remote_path_norm = str(conf_remote_path_norm)\n",
    "\n",
    "normalization_input = {\n",
    "    'setup_local_fs': {'tmp_folder': conf_local_tmp.as_posix()},\n",
    "    'pullremote': conf_remote_path_retiled.as_posix(),\n",
    "    'load': {'attributes': 'all'},\n",
    "    # Filter out artifically high points - give overflow error when writing\n",
    "    'apply_filter': {'filter_type':'select_below',\n",
    "                     'attribute': 'z',\n",
    "                     'threshold': 10000.},  # remove non-physically heigh points\n",
    "    'normalize': 1,\n",
    "    'clear_cache' : {},\n",
    "    'pushremote': conf_remote_path_norm.as_posix(),\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('normalize.json', 'w') as f:\n",
    "    json.dump(normalization_input, f)\n",
    "    \n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "tile = tiles\n",
    "# for tile in tiles:\n",
    "normalization_input_ = copy.deepcopy(normalization_input)\n",
    "normalization_input_['export_point_cloud'] = {'filename': '{}.laz'.format(tile),'overwrite': True}\n",
    "dp = DataProcessing(tile, label=tile).config(normalization_input_).setup_webdav_client(conf_wd_opts)\n",
    "dp.run()\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message='normalization 01-06-22')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5619aac-f36a-4509-8d92-258e2da56122",
   "metadata": {},
   "source": [
    "## Fetching normalized files (tiles) from remote WebDAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4e0ec1-e139-494b-9c76-b8c32cf0e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch norm Tiles 01-06-22\n",
    "remote_path_norm\n",
    "norm_tiles = [t.strip('/') for t in list_remote(get_wdclient(conf_wd_opts), conf_remote_path_norm.as_posix())\n",
    "         if fnmatch.fnmatch(t, 'tile_*_*.laz')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c1c49-e148-4809-8a99-b962e8e5b490",
   "metadata": {},
   "source": [
    "## Extract features - extract defined features from normalized tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52636f93-5279-45a8-9f66-460e300f47c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 12:02:42,438 -           laserfarm.pipeline_remote_data -       INFO - Input dir set to /tmp/tile_287_378_input\n",
      "2022-05-25 12:02:42,439 -           laserfarm.pipeline_remote_data -       INFO - Output dir set to /tmp/tile_287_378_output\n",
      "2022-05-25 12:02:42,440 -           laserfarm.pipeline_remote_data -       INFO - Pulling from WebDAV /webdav/norm/tile_287_378.laz ...\n",
      "2022-05-25 12:02:43,646 -           laserfarm.pipeline_remote_data -       INFO - ... pulling completed.\n",
      "2022-05-25 12:02:43,648 -                laserfarm.data_processing -       INFO - Loading point cloud data ...\n",
      "2022-05-25 12:02:43,648 -                laserfarm.data_processing -       INFO - ... loading /tmp/tile_287_378_input/tile_287_378.laz\n",
      "2022-05-25 12:02:43,735 -                laserfarm.data_processing -       INFO - ... loading completed.\n",
      "2022-05-25 12:02:43,736 -                laserfarm.data_processing -       INFO - Normalizing point-cloud heights ...\n",
      "2022-05-25 12:02:43,763 -                                     root -       INFO - Cylinder size in Bytes: 82802329.25213549\n",
      "2022-05-25 12:02:43,764 -                                     root -       INFO - Memory size in Bytes: 16819240960\n",
      "2022-05-25 12:02:43,764 -                                     root -       INFO - Start tree creation\n",
      "2022-05-25 12:02:43,793 -                                     root -       INFO - Done with env tree creation\n",
      "2022-05-25 12:02:43,796 -                                     root -       INFO - Done with target tree creation\n",
      "2022-05-25 12:02:44,702 -                laserfarm.data_processing -       INFO - ... normalization completed.\n",
      "2022-05-25 12:02:44,703 -                laserfarm.data_processing -       INFO - Filtering point-cloud data\n",
      "2022-05-25 12:02:44,705 -                laserfarm.data_processing -       INFO - Setting up the target grid\n",
      "2022-05-25 12:02:44,706 -                laserfarm.data_processing -       INFO - Checking whether points belong to cell (287,378)\n",
      "2022-05-25 12:02:44,707 -                laserfarm.data_processing -       INFO - Generating target point mesh with 10.0m spacing \n",
      "2022-05-25 12:02:44,708 -                laserfarm.data_processing -       INFO - Building volume of type cell\n",
      "2022-05-25 12:02:44,709 -                laserfarm.data_processing -       INFO - Constructing neighborhoods\n",
      "2022-05-25 12:02:44,710 -                laserfarm.data_processing -       INFO - Starting feature extraction ...\n",
      "2022-05-25 12:02:44,711 -                                     root -       INFO - Cylinder size in Bytes: 6408849013.323179\n",
      "2022-05-25 12:02:44,712 -                                     root -       INFO - Memory size in Bytes: 16819240960\n",
      "2022-05-25 12:02:44,712 -                                     root -       INFO - Start tree creation\n",
      "2022-05-25 12:02:44,713 -                                     root -       INFO - Done with env tree creation\n",
      "2022-05-25 12:02:44,715 -                                     root -       INFO - Done with target tree creation\n",
      "2022-05-25 12:02:44,727 -                                     root -       INFO - Extracting feature(s) \"['perc_95_normalized_height']\"\n",
      "2022-05-25 12:02:44,786 -                                     root -       INFO - Extracting feature(s) \"['perc_95_normalized_height']\" took 0.06 seconds\n",
      "2022-05-25 12:02:44,788 -                laserfarm.data_processing -       INFO - ... feature extraction completed.\n",
      "2022-05-25 12:02:44,789 -                laserfarm.data_processing -       INFO - Exporting target point-cloud ...\n",
      "2022-05-25 12:02:44,790 -                laserfarm.data_processing -       INFO - ... exporting /tmp/tile_287_378_output/perc_95_normalized_height/tile_287_378.ply\n",
      "2022-05-25 12:02:44,899 -                laserfarm.data_processing -       INFO - ... exporting completed.\n",
      "2022-05-25 12:02:44,900 -           laserfarm.pipeline_remote_data -       INFO - Pushing to WebDAV /webdav/targets ...\n",
      "2022-05-25 12:02:47,476 -           laserfarm.pipeline_remote_data -       INFO - ... pushing completed.\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction 01-06-22\n",
    "\n",
    "def send_annotation(start=None,end=None,message=None,tags=None):\n",
    "    if not tags:\n",
    "        tags = []\n",
    "    \n",
    "    tags.append(conf_notebook_name)\n",
    "    \n",
    "    headers = {\n",
    "        'Accept':'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': 'Bearer '+param_grafana_token\n",
    "    }\n",
    "    \n",
    "    data ={\n",
    "      \"time\":start,\n",
    "      \"timeEnd\":end,\n",
    "      \"created\": end,\n",
    "      \"tags\":tags,\n",
    "      \"text\": message\n",
    "    }\n",
    "    resp = requests.post(param_grafana_base_url+'/api/annotations',verify=conf_grafana_verify_ssl,headers=headers,json=data)\n",
    "\n",
    "\n",
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "\n",
    "features = [param_feature_name]\n",
    "\n",
    "tile_mesh_size = float(param_tile_mesh_size)\n",
    "\n",
    "grid_feature = {\n",
    "    'min_x': float(param_min_x),\n",
    "    'max_x': float(param_max_x),\n",
    "    'min_y': float(param_min_y),\n",
    "    'max_y': float(param_max_y),\n",
    "    'n_tiles_side': int(param_n_tiles_side)\n",
    "}\n",
    "\n",
    "feature_extraction_input = {\n",
    "    'setup_local_fs': {'tmp_folder': conf_local_tmp.as_posix()},\n",
    "    'pullremote': conf_remote_path_norm.as_posix(),\n",
    "    'load': {'attributes': [param_attribute]},\n",
    "    'normalize': 1,\n",
    "    'apply_filter': {\n",
    "        'filter_type': param_filter_type, \n",
    "        'attribute': param_attribute,\n",
    "        'value': [int(param_apply_filter_value)]#ground surface (2), water (9), buildings (6), artificial objects (26), and unclassified (1)\n",
    "    },\n",
    "    'generate_targets': {\n",
    "        'tile_mesh_size' : tile_mesh_size,\n",
    "        'validate' : True,\n",
    "        'validate_precision': float(param_validate_precision),\n",
    "        **grid_feature\n",
    "    },\n",
    "    'extract_features': {\n",
    "        'feature_names': features,\n",
    "        'volume_type': 'cell',\n",
    "        'volume_size': tile_mesh_size\n",
    "    },\n",
    "    'export_targets': {\n",
    "        'attributes': features,\n",
    "        'multi_band_files': False\n",
    "    },\n",
    "    'pushremote': conf_remote_path_targets.as_posix(),\n",
    "#     'cleanlocalfs': {}\n",
    "}    \n",
    "\n",
    "t = norm_tiles\n",
    "# for t in norm_tiles:\n",
    "stem, _ = os.path.splitext(t)\n",
    "idx = [int(el) for el in (stem.split('_')[1:])]\n",
    "processing = DataProcessing(t, tile_index=idx,label=stem).config(feature_extraction_input).setup_webdav_client(conf_wd_opts)\n",
    "processing.run()\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message='Feature Extraction 01-06-22')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1374ed-5c88-4686-b266-6e7610471059",
   "metadata": {},
   "source": [
    "## GeoTIFF export - generate GeoTIFF raster layer a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ae4e5f-1a8a-4a50-a221-9302053e8092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 12:02:53,786 -           laserfarm.pipeline_remote_data -       INFO - Input dir set to /tmp/perc_95_normalized_height_input\n",
      "2022-05-25 12:02:53,787 -           laserfarm.pipeline_remote_data -       INFO - Output dir set to /tmp/perc_95_normalized_height_output\n",
      "2022-05-25 12:02:53,788 -           laserfarm.pipeline_remote_data -       INFO - Pulling from WebDAV /webdav/targets/perc_95_normalized_height ...\n",
      "2022-05-25 12:03:17,449 -           laserfarm.pipeline_remote_data -       INFO - ... pulling completed.\n",
      "2022-05-25 12:03:17,452 -                 laserfarm.geotiff_writer -       INFO - 27 PLY files found\n",
      "2022-05-25 12:03:17,508 -                 laserfarm.geotiff_writer -       INFO - No. of points per file: 10000\n",
      "2022-05-25 12:03:17,510 -                 laserfarm.geotiff_writer -       INFO - Resolution: (10.0m x 10.0m)\n",
      "2022-05-25 12:03:17,510 -                 laserfarm.geotiff_writer -       INFO - Splitting data into (1x1) sub-regions\n",
      "2022-05-25 12:03:17,511 -                 laserfarm.geotiff_writer -       INFO - Processing sub-region GeoTiff no. 0 ...\n",
      "2022-05-25 12:03:17,512 -                 laserfarm.geotiff_writer -       INFO - ... number of constituent tiles: 27\n",
      "2022-05-25 12:03:28,479 -                 laserfarm.geotiff_writer -       INFO - ... processing of sub-region completed.\n",
      "2022-05-25 12:03:28,480 -           laserfarm.pipeline_remote_data -       INFO - Pushing to WebDAV /webdav/geotiffs ...\n",
      "2022-05-25 12:03:31,623 -           laserfarm.pipeline_remote_data -       INFO - ... pushing completed.\n",
      "2022-05-25 12:03:31,626 -           laserfarm.pipeline_remote_data -       INFO - Removing input and output folders\n"
     ]
    }
   ],
   "source": [
    "# GeoTIFF Export 01-06-22\n",
    "\n",
    "def send_annotation(start=None,end=None,message=None,tags=None):\n",
    "    if not tags:\n",
    "        tags = []\n",
    "    \n",
    "    tags.append(conf_notebook_name)\n",
    "    \n",
    "    headers = {\n",
    "        'Accept':'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': 'Bearer '+param_grafana_token\n",
    "    }\n",
    "    \n",
    "    data ={\n",
    "      \"time\":start,\n",
    "      \"timeEnd\":end,\n",
    "      \"created\": end,\n",
    "      \"tags\":tags,\n",
    "      \"text\": message\n",
    "    }\n",
    "    resp = requests.post(param_grafana_base_url+'/api/annotations',verify=conf_grafana_verify_ssl,headers=headers,json=data)\n",
    "\n",
    "\n",
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "\n",
    "feature = features\n",
    "\n",
    "remote_path_geotiffs = conf_remote_path_ahn.parent / 'geotiffs'\n",
    "\n",
    "# setup input dictionary to configure the GeoTIFF export pipeline\n",
    "geotiff_export_input = {\n",
    "    'setup_local_fs': {'tmp_folder': conf_local_tmp.as_posix()},\n",
    "    'pullremote': conf_remote_path_targets.as_posix(),\n",
    "    'parse_point_cloud': {},\n",
    "    'data_split': {'xSub': 1, 'ySub': 1},\n",
    "    'create_subregion_geotiffs': {'output_handle': 'geotiff'},\n",
    "    'pushremote': remote_path_geotiffs.as_posix(),\n",
    "    'cleanlocalfs': {}   \n",
    "}\n",
    "\n",
    "writer = GeotiffWriter(input_dir=param_feature_name, bands=param_feature_name,label=param_feature_name).config(geotiff_export_input).setup_webdav_client(conf_wd_opts)\n",
    "writer.run()\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message='GeoTIFF Export 01-06-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "346152b3-b2da-4be7-8137-e9ba2f8f6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/webdav/geotiffs\n"
     ]
    }
   ],
   "source": [
    "print(remote_path_geotiffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c1c23-bb5d-4763-96bb-65ca815909fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d29b2c-e684-49be-83ec-51fb56bf1a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
