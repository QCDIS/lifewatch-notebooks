{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7404fb96",
   "metadata": {},
   "source": "# Laserfarm: LiDAR point cloud analysis for macro-ecology"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "bb8c217415279f91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### User parameters\n",
    "\n",
    "Defines the parameters that can be set by users when executing the workflow."
   ],
   "id": "f4afda19c5108b60"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "# (DO NOT containerize this cell)\n",
    "\n",
    "# Data handling parameters\n",
    "param_minio_server = ''\n",
    "param_bucket_name = ''\n",
    "param_remote_path_root = ''\n",
    "\n",
    "# Laserfarm parameters\n",
    "param_feature_name = 'perc_95_normalized_height'\n",
    "param_validate_precision = '0.001'\n",
    "param_tile_mesh_size = '10.'\n",
    "param_filter_type = 'select_equal'\n",
    "param_attribute = 'raw_classification'\n",
    "param_min_x = '-113107.81'  # EPSG:28992\n",
    "param_max_x = '398892.19'  # EPSG:28992\n",
    "param_min_y = '214783.87'  # EPSG:28992\n",
    "param_max_y = '726783.87'  # EPSG:28992\n",
    "param_n_tiles_side = '512'\n",
    "param_apply_filter_value = '1'\n",
    "param_laz_compression_factor = '7'\n",
    "param_max_filesize_mb = '80'  # desired max file size (in MiB)\n"
   ],
   "id": "27a04045277b08e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dependencies\n",
    "\n",
    "The following cells install extra dependencies that are not included in the Laserfarm flavor by default, and import the libraries used in the notebook."
   ],
   "id": "f68092526ef49c9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# (DO NOT containerize this cell)\n",
    "\n",
    "!pip install minio rasterio"
   ],
   "id": "acc3a2f822afa23b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# (DO NOT containerize this cell)\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from laserfarm import DataProcessing, GeotiffWriter, Retiler\n",
    "from laserfarm.remote_utils import get_wdclient, list_remote\n",
    "from minio import Minio\n",
    "import laspy"
   ],
   "id": "18041536",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Global configuration\n",
    "\n",
    "The following variable are used throughout the code. They are intended to be edited by developers who the notebook."
   ],
   "id": "fdd0baf8fbe0c410"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# (DO NOT containerize this cell)\n",
    "\n",
    "conf_local_tmp = '/tmp/data'\n",
    "conf_local_path_raw = os.path.join(conf_local_tmp, 'raw')\n",
    "conf_local_path_split = os.path.join(conf_local_tmp, 'split')\n",
    "conf_local_path_retiled = os.path.join(conf_local_tmp, 'retiled')\n",
    "conf_local_path_targets = os.path.join(conf_local_tmp, 'targets')\n",
    "conf_local_path_geotiff = os.path.join(conf_local_tmp, 'geotiff')\n",
    "conf_local_path_figures = os.path.join(conf_local_tmp, 'figures')"
   ],
   "id": "bcbff2bb-39bf-4cf9-880d-15e80734b6e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Workflow steps",
   "id": "dd7ea51c2f2e5908"
  },
  {
   "cell_type": "markdown",
   "id": "d03d1b76",
   "metadata": {},
   "source": [
    "### Fetch laz files from remote storage\n",
    "\n",
    "This cell downloads `.laz` files from the remote MinIO storage."
   ]
  },
  {
   "cell_type": "code",
   "id": "573679af",
   "metadata": {
    "trusted": false
   },
   "source": [
    "# S1 Fetch laz files\n",
    "\n",
    "os.makedirs(conf_local_path_raw, exist_ok=True)\n",
    "raw_laz_files = []\n",
    "\n",
    "minio_client = Minio(param_minio_server, secure=True)\n",
    "objects = minio_client.list_objects(\n",
    "    param_bucket_name, prefix=param_remote_path_root\n",
    "    )\n",
    "for obj in objects:\n",
    "    if obj.object_name.lower().endswith('.laz'):\n",
    "        laz_file = os.path.join(\n",
    "            conf_local_path_raw, obj.object_name.split('/')[-1]\n",
    "            )\n",
    "        if not os.path.isfile(laz_file):\n",
    "            print(\n",
    "                f'Downloading {param_bucket_name}:{obj.object_name} to {laz_file}'\n",
    "                )\n",
    "            minio_client.fget_object(\n",
    "                param_bucket_name, obj.object_name, laz_file\n",
    "                )\n",
    "        else:\n",
    "            print(\n",
    "                f'Skipping download of {param_bucket_name}:{obj.object_name} because {laz_file} already exists'\n",
    "                )\n",
    "        raw_laz_files.append(laz_file)\n",
    "\n",
    "print(raw_laz_files)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc0773b1",
   "metadata": {},
   "source": [
    "### Split big files\n",
    "\n",
    "This step splits big laz files into smaller files. This is useful when the original files are too large for the VMs used for the processing. If the VMs have enough memory to hold the laz files, this step can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "id": "c074eba8",
   "metadata": {
    "trusted": false
   },
   "source": [
    "# S2 Split big laz\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def save_chunk_to_laz_file(\n",
    "        in_filename,\n",
    "        out_filename,\n",
    "        offset,\n",
    "        n_points\n",
    "        ):\n",
    "    \"\"\" Read points from a LAS/LAZ file and write them to a new file. \"\"\"\n",
    "    with laspy.open(in_filename) as in_file:\n",
    "        with laspy.open(\n",
    "                out_filename,\n",
    "                mode=\"w\",\n",
    "                header=in_file.header\n",
    "                ) as out_file:\n",
    "            in_file.seek(offset)\n",
    "            points = in_file.read_points(n_points)\n",
    "            out_file.write_points(points)\n",
    "    return out_filename\n",
    "\n",
    "\n",
    "def split_strategy(filename, max_filesize, dest_dir=None):\n",
    "    \"\"\" Set up splitting strategy for a LAS/LAZ file. \"\"\"\n",
    "    with laspy.open(filename) as f:\n",
    "        bytes_per_point = (\n",
    "                f.header.point_format.num_standard_bytes +\n",
    "                f.header.point_format.num_extra_bytes\n",
    "        )\n",
    "        n_points = f.header.point_count\n",
    "    n_points_target = int(\n",
    "        max_filesize * int(param_laz_compression_factor) / bytes_per_point\n",
    "        )\n",
    "    stem, ext = os.path.splitext(filename)\n",
    "    if dest_dir is not None:\n",
    "        stem = os.path.join(dest_dir, os.path.basename(stem))\n",
    "    return [\n",
    "        (filename, f\"{stem}-{n}{ext}\", offset, n_points_target)\n",
    "        for n, offset in enumerate(range(0, n_points, n_points_target))\n",
    "        ]\n",
    "\n",
    "\n",
    "os.makedirs(conf_local_path_split, exist_ok=True)\n",
    "split_laz_files = []\n",
    "\n",
    "for raw_file in raw_laz_files:\n",
    "    inps = split_strategy(\n",
    "        raw_file,\n",
    "        int(param_max_filesize_mb) * 2 ** 20,\n",
    "        dest_dir=conf_local_path_split,\n",
    "        )\n",
    "    print(f'Splitting {raw_file} into {len(inps)} files:')\n",
    "    for inp in inps:\n",
    "        split_file = inp[1]\n",
    "        if not os.path.isfile(split_file):\n",
    "            print(f'  writing {split_file}')\n",
    "            save_chunk_to_laz_file(*inp)\n",
    "        else:\n",
    "            print(f' skipping {split_file} because it already exists')\n",
    "        split_laz_files.append(split_file)\n",
    "\n",
    "print(split_laz_files)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f416e842",
   "metadata": {},
   "source": [
    "### Retile laz files\n",
    "\n",
    "This step splits the laz files into small tiles that can be easily processed."
   ]
  },
  {
   "cell_type": "code",
   "id": "8b489f30",
   "metadata": {
    "trusted": false
   },
   "source": [
    "# S3 Retile laz files\n",
    "\n",
    "grid_retile = {\n",
    "    'min_x': float(param_min_x),\n",
    "    'max_x': float(param_max_x),\n",
    "    'min_y': float(param_min_y),\n",
    "    'max_y': float(param_max_y),\n",
    "    'n_tiles_side': int(param_n_tiles_side),\n",
    "    }\n",
    "\n",
    "retiling_input = {\n",
    "    'setup_local_fs': {\n",
    "        'input_folder': conf_local_path_split,\n",
    "        'output_folder': conf_local_path_retiled,\n",
    "        },\n",
    "    'set_grid': grid_retile,\n",
    "    'split_and_redistribute': {},\n",
    "    'validate': {},\n",
    "    }\n",
    "\n",
    "os.makedirs(conf_local_path_retiled, exist_ok=True)\n",
    "tiles = []\n",
    "\n",
    "for file in split_laz_files:\n",
    "    base_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    retile_record_filename = os.path.join(\n",
    "        conf_local_path_retiled,\n",
    "        f'{base_name}_retile_record.js',\n",
    "        )\n",
    "    if not os.path.isfile(retile_record_filename):\n",
    "        print(f'Retiling {file}')\n",
    "        retiler = Retiler(file, label=file).config(retiling_input)\n",
    "        retiler.run()\n",
    "    else:\n",
    "        print(\n",
    "            f'Skipping retiling of {file} because {retile_record_filename} already exists'\n",
    "            )\n",
    "    # load filenames from retile record\n",
    "    with open(retile_record_filename, 'r') as f:\n",
    "        retile_record = json.load(f)\n",
    "    tiles += retile_record['redistributed_to']\n",
    "\n",
    "print(tiles)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Unique tiles\n",
    "\n",
    "Ensure that the list of tiles contains no duplicates. This is necessary because the above step might return duplicates. If no splitting is used, this cell can be merged with the above one. "
   ],
   "id": "929607bce55d77f2"
  },
  {
   "cell_type": "code",
   "id": "c8814681",
   "metadata": {
    "trusted": false
   },
   "source": "# S4 Unique tiles\n\ntiles = list(set(tiles))\n\nprint(tiles)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extract features from tiles\n",
    "\n",
    "Run the feature extraction for each tile. The features are extracted using [laserchicken](https://github.com/eEcoLiDAR/laserchicken)."
   ],
   "id": "45891c72fb134376"
  },
  {
   "cell_type": "code",
   "id": "ab006ce8-f290-414d-a1be-1dcd3b2f101c",
   "metadata": {
    "trusted": false
   },
   "source": [
    "# S5 Extract features\n",
    "\n",
    "feature_files = []\n",
    "\n",
    "for i, tile in enumerate(tiles):\n",
    "    grid_feature = {\n",
    "        'min_x': float(param_min_x),\n",
    "        'max_x': float(param_max_x),\n",
    "        'min_y': float(param_min_y),\n",
    "        'max_y': float(param_max_y),\n",
    "        'n_tiles_side': int(param_n_tiles_side),\n",
    "        }\n",
    "\n",
    "    feature_extraction_input = {\n",
    "        'setup_local_fs': {\n",
    "            'input_folder': conf_local_path_retiled,\n",
    "            'output_folder': conf_local_path_targets,\n",
    "            },\n",
    "        'load': {'attributes': [param_attribute]},\n",
    "        'normalize': 1,\n",
    "        'apply_filter': {\n",
    "            'filter_type': param_filter_type,\n",
    "            'attribute': param_attribute,\n",
    "            'value': [int(param_apply_filter_value)],\n",
    "            #ground surface (2), water (9), buildings (6), artificial objects (26), vegetation (?), and unclassified (1)\n",
    "            },\n",
    "        'generate_targets': {\n",
    "            'tile_mesh_size': float(param_tile_mesh_size),\n",
    "            'validate': True,\n",
    "            'validate_precision': float(param_validate_precision),\n",
    "            **grid_feature\n",
    "            },\n",
    "        'extract_features': {\n",
    "            'feature_names': [param_feature_name],\n",
    "            'volume_type': 'cell',\n",
    "            'volume_size': float(param_tile_mesh_size),\n",
    "            },\n",
    "        'export_targets': {\n",
    "            'attributes': [param_feature_name],\n",
    "            'multi_band_files': False,\n",
    "            },\n",
    "        }\n",
    "    idx = (tile.split('_')[1:])\n",
    "\n",
    "    target_file = os.path.join(\n",
    "        conf_local_path_targets, param_feature_name, tile + '.ply'\n",
    "        )\n",
    "    print(target_file)\n",
    "\n",
    "    if not os.path.isfile(target_file):\n",
    "        print(f'Extracting features from {tile} ({i + 1} of {len(tiles)})')\n",
    "        processing = DataProcessing(tile, tile_index=idx, label=tile).config(\n",
    "            feature_extraction_input\n",
    "            )\n",
    "        processing.run()\n",
    "    else:\n",
    "        print(\n",
    "            f'Skipping features extraction for {tile} ({i + 1} of {len(tiles)}) because {target_file} already exists'\n",
    "            )\n",
    "\n",
    "    feature_files.append(target_file)\n",
    "\n",
    "print(feature_files)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Save GeoTIFF\n",
    "\n",
    "Generate a GeoTIFF containing the extracted features. This merges all tiles into one file. "
   ],
   "id": "2f31ac2f011d4eff"
  },
  {
   "cell_type": "code",
   "id": "7f43c05f-cdd2-44e6-b745-119832689353",
   "metadata": {
    "trusted": false
   },
   "source": [
    "# S6 Save GeoTIFF\n",
    "\n",
    "print(feature_files)\n",
    "\n",
    "geotiff_export_input = {\n",
    "    'setup_local_fs': {\n",
    "        'input_folder': conf_local_path_targets,\n",
    "        'output_folder': conf_local_path_geotiff,\n",
    "        },\n",
    "    'parse_point_cloud': {},\n",
    "    'data_split': {\n",
    "        'xSub': 1,\n",
    "        'ySub': 1,\n",
    "        },\n",
    "    'create_subregion_geotiffs': {\n",
    "        'output_handle': 'geotiff'\n",
    "        },\n",
    "    }\n",
    "\n",
    "writer = (\n",
    "    GeotiffWriter(\n",
    "        input_dir=param_feature_name,\n",
    "        bands=param_feature_name,\n",
    "        label=param_feature_name,\n",
    "        )\n",
    "    .config(geotiff_export_input)\n",
    "    )\n",
    "writer.run()\n",
    "\n",
    "geo_tiff = os.path.join(\n",
    "    conf_local_path_geotiff,\n",
    "    f'geotiff_TILE_000_BAND_{param_feature_name}.tif'\n",
    "    )\n",
    "print(geo_tiff)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create figures",
   "id": "55168359c67edc54"
  },
  {
   "cell_type": "code",
   "id": "0dda61ce-ae9a-4d73-bf5b-b6b929d5d023",
   "metadata": {
    "trusted": false
   },
   "source": [
    "# S7 Figures\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import rasterio.plot as rp\n",
    "\n",
    "src = rasterio.open(geo_tiff)\n",
    "\n",
    "os.makedirs(conf_local_path_figures, exist_ok=True)\n",
    "\n",
    "ax = plt.gca()\n",
    "rio_plot = rp.show((src, 1), interpolation='none', ax=ax)\n",
    "img = rio_plot.get_images()[0]\n",
    "cb = plt.colorbar(img, ax=ax)\n",
    "cb.set_label(f'{param_feature_name}')\n",
    "plt.xlabel('EPSG:28992 X [m]')\n",
    "plt.ylabel('EPSG:28992 Y [m]')\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(conf_local_path_figures, f'{param_feature_name}_map.pdf'))\n",
    "\n",
    "rp.show_hist(\n",
    "    src,\n",
    "    bins=50,\n",
    "    lw=0.0,\n",
    "    stacked=False,\n",
    "    alpha=0.3,\n",
    "    histtype='stepfilled',\n",
    "    title=\"Histogram\",\n",
    "    )\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(conf_local_path_figures, f'{param_feature_name}_histogram.pdf'))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:laserfarm]",
   "language": "python",
   "name": "conda-env-laserfarm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
