{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grafana password: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "grafana_pwd = getpass.getpass('grafana password: ')\n",
    "grafana_base_url = 'https://20.85.222.132:3000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def send_annotation(start=None,end=None,message=None):\n",
    "    tag_name = \"laserfarm\"\n",
    "    headers = {\n",
    "        'Accept':'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    \n",
    "    data ={\n",
    "      \"dashboardId\":1,\n",
    "    #   \"panelId\":8,\n",
    "      \"time\":start,\n",
    "      \"timeEnd\":end,\n",
    "      \"created\": end,\n",
    "      \"tags\":[tag_name],\n",
    "      \"text\": message\n",
    "    }\n",
    "    end = int(round(time.time() * 1000))\n",
    "    resp = requests.post(grafana_base_url+'/api/annotations',verify=False,auth=('admin', grafana_pwd),headers=headers,json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: dask in /opt/conda/lib/python3.8/site-packages (2021.5.1)\n",
      "Requirement already satisfied: distributed in /opt/conda/lib/python3.8/site-packages (2021.5.1)\n",
      "Requirement already satisfied: laserfarm in /opt/conda/lib/python3.8/site-packages (0.1.3)\n",
      "Requirement already satisfied: azure-storage-blob in /opt/conda/lib/python3.8/site-packages (12.8.1)\n",
      "Requirement already satisfied: msrest>=0.6.18 in /opt/conda/lib/python3.8/site-packages (from azure-storage-blob) (0.6.21)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /opt/conda/lib/python3.8/site-packages (from azure-storage-blob) (3.4.7)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.10.0 in /opt/conda/lib/python3.8/site-packages (from azure-storage-blob) (1.15.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.10.0->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.10.0->azure-storage-blob) (2.25.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.8/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.20)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from msrest>=0.6.18->azure-storage-blob) (1.3.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from msrest>=0.6.18->azure-storage-blob) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from msrest>=0.6.18->azure-storage-blob) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.10.0->azure-storage-blob) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.10.0->azure-storage-blob) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.10.0->azure-storage-blob) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-storage-blob) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from dask) (2021.5.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/conda/lib/python3.8/site-packages (from dask) (0.11.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from dask) (5.4.1)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from dask) (1.6.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /opt/conda/lib/python3.8/site-packages (from dask) (1.2.0)\n",
      "Requirement already satisfied: locket in /opt/conda/lib/python3.8/site-packages (from partd>=0.3.10->dask) (0.2.1)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from distributed) (1.0.2)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from distributed) (1.7.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from distributed) (49.6.0.post20210108)\n",
      "Requirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed) (5.8.0)\n",
      "Requirement already satisfied: click>=6.6 in /opt/conda/lib/python3.8/site-packages (from distributed) (7.1.2)\n",
      "Requirement already satisfied: zict>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from distributed) (2.0.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /opt/conda/lib/python3.8/site-packages (from distributed) (2.4.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.8/site-packages (from distributed) (6.1)\n",
      "Requirement already satisfied: heapdict in /opt/conda/lib/python3.8/site-packages (from zict>=0.1.3->distributed) (1.0.1)\n",
      "Requirement already satisfied: plyfile in /opt/conda/lib/python3.8/site-packages (from laserfarm) (0.7.2)\n",
      "Requirement already satisfied: asyncssh in /opt/conda/lib/python3.8/site-packages (from laserfarm) (2.6.0)\n",
      "Requirement already satisfied: webdavclient3 in /opt/conda/lib/python3.8/site-packages (from laserfarm) (3.14.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from laserfarm) (1.20.3)\n",
      "Requirement already satisfied: pylas in /opt/conda/lib/python3.8/site-packages (from laserfarm) (0.4.3)\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.8/site-packages (from laserfarm) (6.2.2)\n",
      "Requirement already satisfied: PyShp in /opt/conda/lib/python3.8/site-packages (from laserfarm) (2.1.3)\n",
      "Requirement already satisfied: lazperf in /opt/conda/lib/python3.8/site-packages (from laserfarm) (1.5)\n",
      "Requirement already satisfied: fire in /opt/conda/lib/python3.8/site-packages (from laserfarm) (0.4.0)\n",
      "Requirement already satisfied: laserchicken>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from laserfarm) (0.4.2)\n",
      "Requirement already satisfied: shapely in /opt/conda/lib/python3.8/site-packages (from laserfarm) (1.7.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from laserchicken>=0.4.1->laserfarm) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from laserchicken>=0.4.1->laserfarm) (2.8.1)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.8/site-packages (from laserchicken>=0.4.1->laserfarm) (0.4.4)\n",
      "Requirement already satisfied: mock in /opt/conda/lib/python3.8/site-packages (from laserchicken>=0.4.1->laserfarm) (4.0.3)\n",
      "Requirement already satisfied: scipy>=0.11 in /opt/conda/lib/python3.8/site-packages (from laserchicken>=0.4.1->laserfarm) (1.6.0)\n",
      "Requirement already satisfied: bokeh!=2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from dask) (2.3.2)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /opt/conda/lib/python3.8/site-packages (from bokeh!=2.0.0,>=1.0.0->dask) (8.1.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/conda/lib/python3.8/site-packages (from bokeh!=2.0.0,>=1.0.0->dask) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.8/site-packages (from bokeh!=2.0.0,>=1.0.0->dask) (3.10.0.0)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /opt/conda/lib/python3.8/site-packages (from bokeh!=2.0.0,>=1.0.0->dask) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2>=2.9->bokeh!=2.0.0,>=1.0.0->dask) (2.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=16.8->bokeh!=2.0.0,>=1.0.0->dask) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->laserchicken>=0.4.1->laserfarm) (2021.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.8/site-packages (from fire->laserfarm) (1.1.0)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.8/site-packages (from pytest->laserfarm) (0.10.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.8/site-packages (from pytest->laserfarm) (21.2.0)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /opt/conda/lib/python3.8/site-packages (from pytest->laserfarm) (0.13.1)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.8/site-packages (from pytest->laserfarm) (1.1.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.8/site-packages (from pytest->laserfarm) (1.10.0)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.8/site-packages (from webdavclient3->laserfarm) (4.6.3)\n",
      "\n",
      "CondaValueError: too few arguments, must supply command line package specs or --file\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host '20.85.222.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "!pip install dask distributed laserfarm azure-storage-blob\n",
    "!conda install --yes -c gdal\n",
    "!conda upgrade --yes numpy\n",
    "!conda install -y -c conda-forge python-pdal \n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"install reqirements\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "import requests\n",
    "\n",
    "\n",
    "                    \n",
    "from dask.distributed import LocalCluster, SSHCluster \n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote\n",
    "\n",
    "def last_modified(opts, remote_path):\n",
    "    info = get_info_remote(get_wdclient(opts), remote_path.as_posix())\n",
    "    format_ = '%a, %d %b %Y %H:%M:%S GMT'\n",
    "    return datetime.datetime.strptime(info['modified'], format_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-Pipeline Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Run-Specific Input\n",
    "\n",
    "    Fill in the tokens for the azure data storage. LAZ files updated since the last workflow run will be re-run through the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebDAV username: a\n",
      "WebDAV password: ········\n",
      "Date last run (YYYY-MM-DD): 2019-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host '20.85.222.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "webdav_login = input('WebDAV username: ')\n",
    "webdav_password = getpass.getpass('WebDAV password: ')\n",
    "last_run = datetime.datetime.strptime(input('Date last run (YYYY-MM-DD): '), '%Y-%m-%d')\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"Set Run-Specific Input\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Connection to Remote Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host '20.85.222.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "remote_path_root = pathlib.Path('/ahn3_current_TOP10NL_ud20200323/')\n",
    "wd_opts = {\n",
    "    'webdav_hostname': 'http://13.79.161.201:8080/',\n",
    "    'webdav_login': webdav_login,\n",
    "    'webdav_password': webdav_password\n",
    "}\n",
    "assert get_wdclient(wd_opts).check(remote_path_root.as_posix())\n",
    "\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"Check Connection to Remote Storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cluster\n",
    "\n",
    "Setup Dask cluster used for all the macro-pipeline calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host '20.85.222.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "local_tmp = pathlib.Path('/tmp')\n",
    "cluster = LocalCluster(processes=True, \n",
    "                       n_workers=2, \n",
    "                       threads_per_worker=1, \n",
    "                       local_directory=local_tmp/'dask-worker-space')\n",
    "# nprocs_per_node = 2\n",
    "# cluster = SSHCluster(hosts=['172.17.0.2', \n",
    "#                             '172.17.0.2', \n",
    "#                             '172.17.0.3'], \n",
    "#                      connect_options={'known_hosts': None, \n",
    "#                                       'username': 'ubuntu', \n",
    "#                                       'client_keys': '/home/ubuntu/.ssh/id_rsa'},\n",
    "#                      worker_options={'nthreads': 1, \n",
    "#                                      'nprocs': nprocs_per_node,\n",
    "#                                      'local_directory': local_tmp/'dask-worker-space'}, \n",
    "#                      scheduler_options={'dashboard_address': '8787'})\n",
    "cluster\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"Setup Dask cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retiling\n",
    "\n",
    "The raw point-cloud files are downloaded and retiled to a regular grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve and retile: 1 LAZ files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host '20.85.222.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "# dCache path to raw LAZ files \n",
    "remote_path_ahn = remote_path_root / 'Test_las'\n",
    "\n",
    "# dCache path where to copy retiled PLY files\n",
    "remote_path_retiled = remote_path_ahn.parent / 'retiled'\n",
    "\n",
    "# details of the retiling schema\n",
    "grid = {\n",
    "    'min_x': -113107.81,\n",
    "    'max_x': 398892.19,\n",
    "    'min_y': 214783.87,\n",
    "    'max_y': 726783.87,\n",
    "    'n_tiles_side': 512\n",
    "}\n",
    "\n",
    "\n",
    "# determine which LAZ files have been updated since the last run \n",
    "laz_files = [f for f in list_remote(get_wdclient(wd_opts), remote_path_ahn.as_posix())\n",
    "             if f.lower().endswith('.laz') and last_modified(wd_opts, remote_path_ahn/f) > last_run]\n",
    "print('Retrieve and retile: {} LAZ files'.format(len(laz_files)))\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"Retiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host '20.85.222.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "# setup input dictionary to configure the retiling pipeline\n",
    "retiling_input = {\n",
    "    'setup_local_fs': {'tmp_folder': local_tmp.as_posix()},\n",
    "    'pullremote': remote_path_ahn.as_posix(),\n",
    "    'set_grid': grid,\n",
    "    'split_and_redistribute': {},\n",
    "    'validate': {},\n",
    "    'pushremote': remote_path_retiled.as_posix(),\n",
    "    'cleanlocalfs': {}\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('retiling.json', 'w') as f:\n",
    "    json.dump(retiling_input, f)\n",
    "    \n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"retiling_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "macro = MacroPipeline()\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "macro.tasks = [Retiler(file).config(retiling_input).setup_webdav_client(wd_opts) for file in laz_files]\n",
    "macro.set_labels([os.path.splitext(file)[0] for file in laz_files])\n",
    "\n",
    "macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run! \n",
    "macro.run()\n",
    "\n",
    "# save outcome results and check that no error occurred before continuing\n",
    "macro.print_outcome(to_file='retiling.out')\n",
    "assert not macro.get_failed_pipelines()\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"MacroPipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Features computed for the retiled point-cloud data are assigned to a regular 'target' grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve and process: 2 tiles\n"
     ]
    }
   ],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "# target mesh size & list of features\n",
    "tile_mesh_size = 10.\n",
    "features = ['perc_95_normalized_height', 'pulse_penetration_ratio', 'entropy_normalized_height', 'point_density']\n",
    "\n",
    "# dCache path where to copy the feature-enriched target data\n",
    "remote_path_targets = remote_path_ahn.parent / 'targets'\n",
    "\n",
    "# determine which tiles have been updated since last run, and extract tile index numbers\n",
    "tiles = [t.strip('/') for t in list_remote(get_wdclient(wd_opts), remote_path_retiled.as_posix())\n",
    "         if fnmatch.fnmatch(t, 'tile_*_*/') and last_modified(wd_opts, remote_path_retiled/t) > last_run]\n",
    "tile_indices = [[int(el) for el in tile.split('_')[1:]] for tile in tiles]\n",
    "print('Retrieve and process: {} tiles'.format(len(tile_indices)))\n",
    "\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"Feature Extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "# setup input dictionary to configure the feature extraction pipeline\n",
    "feature_extraction_input = {\n",
    "    'setup_local_fs': {'tmp_folder': local_tmp.as_posix()},\n",
    "    'pullremote': remote_path_retiled.as_posix(),\n",
    "    'load': {'attributes': ['raw_classification']},\n",
    "    'normalize': 1,\n",
    "    'apply_filter': {\n",
    "        'filter_type': 'select_equal', \n",
    "        'attribute': 'raw_classification',\n",
    "        'value': [1, 6]#ground surface (2), water (9), buildings (6), artificial objects (26), vegetation (?), and unclassified (1)\n",
    "    },\n",
    "    'generate_targets': {\n",
    "        'tile_mesh_size' : tile_mesh_size,\n",
    "        'validate' : True,\n",
    "        **grid\n",
    "    },\n",
    "    'extract_features': {\n",
    "        'feature_names': features,\n",
    "        'volume_type': 'cell',\n",
    "        'volume_size': tile_mesh_size\n",
    "    },\n",
    "    'export_targets': {\n",
    "        'attributes': features,\n",
    "        'multi_band_files': False\n",
    "    },\n",
    "    'pushremote': remote_path_targets.as_posix(),\n",
    "#     'cleanlocalfs': {}\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('feature_extraction.json', 'w') as f:\n",
    "    json.dump(feature_extraction_input, f)\n",
    "    \n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"feature_extraction.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6109bcff04e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# save outcome results and check that no error occurred before continuing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmacro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_outcome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'feature_extraction.out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmacro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_failed_pipelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "macro = MacroPipeline()\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "macro.tasks = [DataProcessing(t, tile_index=idx).config(feature_extraction_input).setup_webdav_client(wd_opts) \n",
    "               for t, idx in zip(tiles, tile_indices)]\n",
    "macro.set_labels(tiles)\n",
    "\n",
    "macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "macro.run()\n",
    "\n",
    "# save outcome results and check that no error occurred before continuing\n",
    "macro.print_outcome(to_file='feature_extraction.out')\n",
    "assert not macro.get_failed_pipelines()\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"MacroPipeline2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoTIFF Export\n",
    "\n",
    "Export the rasterized features from the target grid to GeoTIFF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "\n",
    "# dCache path where to copy the GeoTIFF files\n",
    "remote_path_geotiffs = remote_path_ahn.parent / 'geotiffs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup input dictionary to configure the GeoTIFF export pipeline\n",
    "geotiff_export_input = {\n",
    "    'setup_local_fs': {'tmp_folder': local_tmp.as_posix()},\n",
    "    'pullremote': remote_path_targets.as_posix(),\n",
    "    'parse_point_cloud': {},\n",
    "    'data_split': {'xSub': 1, 'ySub': 1},\n",
    "    'create_subregion_geotiffs': {'output_handle': 'geotiff'},\n",
    "    'pushremote': remote_path_geotiffs.as_posix(),\n",
    "    'cleanlocalfs': {}   \n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('geotiff_export.json', 'w') as f:\n",
    "    json.dump(geotiff_export_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2c37e4c08b75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# save outcome results and check that no error occurred before continuing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmacro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_outcome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'geotiff_export.out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmacro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_failed_pipelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "macro = MacroPipeline()\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "macro.tasks = [GeotiffWriter(input_dir=feature, bands=feature).config(geotiff_export_input).setup_webdav_client(wd_opts) \n",
    "               for feature in features]\n",
    "macro.set_labels(features)\n",
    "\n",
    "macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "macro.run()\n",
    "\n",
    "# save outcome results and check that no error occurred before continuing\n",
    "macro.print_outcome(to_file='geotiff_export.out')\n",
    "assert not macro.get_failed_pipelines()\n",
    "\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"GeoTIFF Export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "start = int(round(time.time() * 1000))\n",
    "cluster.close()\n",
    "end = int(round(time.time() * 1000))\n",
    "send_annotation(start=start,end=end,message=\"cluster.close()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
