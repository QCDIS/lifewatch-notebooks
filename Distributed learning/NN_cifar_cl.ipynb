{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba92b47-e20e-483d-b8bf-ed9750905a44",
   "metadata": {},
   "source": [
    "## Centralized Learning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693436f7-0ca5-4bf2-9801-4b1b3f4e26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_cifar_cl\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchvision.transforms as transforms\n",
    "from torch import Tensor\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import csv  \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "DATA_ROOT = \"./data/cifar-10\"\n",
    "BATCH_SIZE = BATCH_SIZE\n",
    "epochs = epochs\n",
    "\n",
    "def load_data() -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, Dict]:\n",
    "    \"\"\"Load CIFAR-10 (training and test set).\"\"\"\n",
    "    \n",
    "    ld_start = time.time()\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(DATA_ROOT, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    testset = CIFAR10(DATA_ROOT, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    num_examples = {\"trainset\" : len(trainset), \"testset\" : len(testset)}\n",
    "    ld_end = time.time()\n",
    "    load_data_time = ld_end-ld_start\n",
    "    print(\"Time to load data: \", load_data_time, \"s\")\n",
    "\n",
    "    return trainloader, testloader, num_examples, load_data_time\n",
    "\n",
    "def train(\n",
    "    net: Net,\n",
    "    trainloader: torch.utils.data.DataLoader,\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    \"\"\"Train the network.\"\"\"\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    print(f\"Training {epochs} epoch(s) w/ {len(trainloader)} batches each\")\n",
    "    \n",
    "    start_train = time.time()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    # Train the network\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        print(\"Epoch \", epoch+1)\n",
    "        \n",
    "        correct, total, train_loss_epoch = 0, 0, 0.0\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics: loss\n",
    "            running_loss += loss.item()\n",
    "            # if i % 100 == 99:  # print every 100 mini-batches\n",
    "                # print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                # running_loss = 0.0\n",
    "        # print statistics: running_loss, accuracy per epoch in training\n",
    "        running_loss = running_loss / total\n",
    "        train_acc_epoch = correct / total\n",
    "        # val_loss_epoch, val_acc_epoch = test(net, valloader)\n",
    "        info = \"[INFO] Epoch {}/{} - train_loss: {:.6f} - train_acc: {:.6f} \".format(\n",
    "                epoch + 1, epochs, running_loss, train_acc_epoch)\n",
    "        print(info + \"\\n\")\n",
    "        train_loss.append(running_loss)\n",
    "        train_acc.append(train_acc_epoch)\n",
    "    print(\"Data of running loss: \", train_loss)\n",
    "    print(\"Data of running accuracy: \", train_acc)\n",
    "    end_train = time.time()\n",
    "    train_time = end_train - start_train\n",
    "    print(\"Time to train the whole network: \", train_time, \" s\")\n",
    "    return train_time, train_loss, train_acc\n",
    "\n",
    "\n",
    "def test(\n",
    "    net: Net,\n",
    "    testloader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0.0\n",
    "    # whole_labels, whole_predicted = torch.Tensor([]), torch.Tensor([])\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images = torch.from_numpy(np.asarray(data[0]).astype('float32'))\n",
    "            images, labels = images.to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # whole_labels = torch.cat((whole_labels.cpu(), labels.cpu()))\n",
    "            # whole_predicted = torch.cat((whole_predicted.cpu(), predicted.cpu()))\n",
    "    # print(\"CONFUSION MATRIX:\")\n",
    "    # print(confusion_matrix(whole_labels.cpu(), whole_predicted.cpu()))\n",
    "    accuracy = correct / total\n",
    "    loss = loss / total\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "\"\"\"Run program and data collection\"\"\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NAME = torch.cuda.get_device_name(DEVICE)\n",
    "print(\"Centralized PyTorch training using: \", DEVICE, NAME)\n",
    "\n",
    "print(\"Load data\")\n",
    "# trainloader, testloader, _ = load_data()\n",
    "trainloader, testloader, num_examples, load_data_time = load_data()\n",
    "    \n",
    "print(\"Start training\")\n",
    "net=Net().to(DEVICE)\n",
    "train_time, train_loss, train_acc= train(net=net, trainloader=trainloader, epochs=epochs, device=DEVICE)\n",
    "print(\"Evaluate model\")\n",
    "loss, accuracy = test(net=net, testloader=testloader, device=DEVICE)\n",
    "print(\"Batch size: \", BATCH_SIZE)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "header = ['batch_size', 'epochs', 'load_data_time', 'train_time', 'loss(test)', 'ACC(test)', 'train_loss', 'train_acc', 'DEVICE', 'NAME']\n",
    "data = [BATCH_SIZE, epochs, load_data_time, train_time, loss, accuracy, train_loss, train_acc, DEVICE, NAME]\n",
    "\n",
    "with open('./res_model.csv', 'a', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    # writer.writerow(header)\n",
    "\n",
    "    # write the data\n",
    "    writer.writerow(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
